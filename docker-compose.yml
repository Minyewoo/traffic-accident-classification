version: "3.9"
services:
  spark:
    hostname: spark
    image: bitnami/spark:3.3.1
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark:7077
    ports:
      - ${SAPRK_WEBUI_EXTERNAL_PORT}:8080
    networks:
      - spark_net

  spark-worker:
    image: bitnami/spark:3.3.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
    networks:
      - spark_net
      - hdfs

  rabbitmq:
    hostname: rabbitmq
    image: rabbitmq:3.9.25-management-alpine
    ports:
      - 5672:5672
      - 15672:15672
    volumes:
      - ./data/rabbitmq/:/var/lib/rabbitmq/
    networks:
      - rabbitmq_net
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 300s
      timeout: 30s
      retries: 5
    
  redis:
    hostname: redis
    image: nyaaneet/rejson:v2.4.0
    ports:
      - 6379:6379
    networks:
      - redis_net
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 300s
      timeout: 30s
      retries: 5

  mongo:
    hostname: mongo
    image: bitnami/mongodb:6.0.3
    environment:
      - MONGODB_ROOT_USER=${MONGODB_ROOT_USER}
      - MONGODB_ROOT_PASSWORD=${MONGODB_ROOT_PASSWORD}
      - MONGODB_DATABASE=${MONGODB_DATABASE}
    ports:
      - ${MONGODB_EXTERNAL_PORT}:27017
    networks:
      - mongo_net

  hdfs-namenode:
    image: gradiant/hdfs:3.2.2
    volumes:
      - ./data/hadoop_namenode:/hadoop/dfs/name
    environment:
      - HDFS_CONF_dfs_permissions_enabled=false
    command:
      - namenode
    ports:
      - ${HDFS_EXTERNAL_PORT}:8020
      - ${HDFS_WEBUI_EXTERNAL_PORT}:9870
    networks:
      - hdfs

  hdfs-datanode:
    image: gradiant/hdfs:3.2.2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - ./data/hadoop_datanode:/hadoop/dfs/data
    command:
      - datanode
    networks:
      - hdfs
  
  traffic_events_crawler_msc:
    env_file:
      - ./config/crawler.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=traffic_events
      - TRAFFIC_URL=${TRAFFIC_MSC_URL}
      - CITY_NAME=msc
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  weather_forecast_crawler_msc:
    env_file:
      - ./config/crawler.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=weather_forecast
      - WFORECAST_URL=${WFORECAST_MSC_URL}
      - CITY_NAME=msc
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  traffic_events_crawler_spb:
    env_file:
      - ./config/crawler.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=traffic_events
      - TRAFFIC_URL=${TRAFFIC_SPB_URL}
      - CITY_NAME=spb
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  weather_forecast_crawler_spb:
    env_file:
      - ./config/crawler.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=weather_forecast
      - WFORECAST_URL=${WFORECAST_SPB_URL}
      - CITY_NAME=spb
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  scheduler:
    env_file:
      - ./config/shceduler.env
    environment:
      - RABBITMQ_HOST=rabbitmq
      - PROCESSING_SCHEDULER_EXCHANGE_NAME=${PROCESSING_SCHEDULER_EXCHANGE_NAME}
      - CRAWLING_SCHEDULER_EXCHANGE_NAME=${CRAWLING_SCHEDULER_EXCHANGE_NAME}
    build:
      context: ./scheduler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
    networks:
      - rabbitmq_net

  data-preprocessor:
    build:
      context: ./data-preprocessor/
      dockerfile: Dockerfile
    volumes:
      - ./data-preprocessor/src:/app:ro
    environment:
      - PROCESSING_SCHEDULER_EXCHANGE_NAME=${PROCESSING_SCHEDULER_EXCHANGE_NAME}
      - SPARK_SUBNET=${SPARK_SUBNET}
      - HDFS_URL=hdfs://hdfs-namenode:8020
      - HDFS_BLOCK_SIZE=${HDFS_BLOCK_SIZE}
      - TRAFFIC_DATA_SAVING_PATH=processed/data-traffic.csv
      - TRAFFIC_COUNT_DATA_SAVING_PATH=${TRAFFIC_COUNT_DATA_SAVING_PATH}
    restart: on-failure
    networks:
      - spark_net
      - rabbitmq_net
      - redis_net
      - hdfs

  model:
    build:
      context: ./model/
      dockerfile: Dockerfile
    environment:
      - RABBITMQ_HOST=rabbitmq
      - TRAINING_SCHEDULER_EXCHANGE_NAME=training_schedule
      - SPARK_SUBNET=${SPARK_SUBNET}
      - HDFS_URL=hdfs://hdfs-namenode:8020
      - HDFS_BLOCK_SIZE=${HDFS_BLOCK_SIZE}
      - TRAFFIC_COUNT_DATA_SAVING_PATH=${TRAFFIC_COUNT_DATA_SAVING_PATH}
      - TRAFFIC_BEST_MODEL_SAVING_PATH=weights/best_weights
    restart: on-failure
    networks:
      - spark_net
      - rabbitmq_net
      - hdfs

  inference:
    build:
      context: ./inference/
      dockerfile: Dockerfile
    env_file:
      - ./config/crawler.env
    environment:
      - RABBITMQ_HOST=rabbitmq
      - INFERENCE_SCHEDULER_EXCHANGE_NAME=inference_schedule
      - SPARK_SUBNET=${SPARK_SUBNET}
      - HDFS_URL=hdfs://hdfs-namenode:8020
      - HDFS_BLOCK_SIZE=${HDFS_BLOCK_SIZE}
      - TRAFFIC_BEST_MODEL_SAVING_PATH=weights/best_weights
      - WFORECAST_URLS={"msc":"https://yandex.ru/pogoda/moscow","spb":"https://yandex.ru/pogoda/saint-petersburg"}
      - MONGODB_HOST=mongo
      - MONGODB_PORT=27017
      - MONGODB_USERNAME=${MONGODB_ROOT_USER}
      - MONGODB_PASSWORD=${MONGODB_ROOT_PASSWORD}
      - MONGODB_DATABASE=traffic
    restart: on-failure
    networks:
      - spark_net
      - rabbitmq_net
      - hdfs
      - mongo_net
    
  web-app-backend:
    build:
      context: ./web-app-back/
      dockerfile: Dockerfile
    environment:
      - MONGO_HOST=mongo
      - MONGO_USERNAME=${MONGODB_ROOT_USER}
      - MONGO_PASSWORD=${MONGODB_ROOT_PASSWORD}
      - DB_NAME=${MONGODB_DATABASE}
      - DB_COLLECTION=predictions
    restart: on-failure
    ports:
      - ${BACKEND_EXTERNAL_PORT}:8080
    networks:
      - mongo_net
      
  web-app-frontend:
    build:
      context: ./web-app-front/
      dockerfile: Dockerfile
      args:
        REACT_APP_API_URL: ${BACKEND_EXTERNAL_HOST}:${BACKEND_EXTERNAL_PORT}
    ports:
      - ${FRONTEND_EXTERNAL_PORT}:80

networks:
  spark_net:
    driver: bridge
    ipam:
      config:
        - subnet: ${SPARK_SUBNET}
          gateway: ${SPARK_GATEAWAY}
  
  redis_net:
    driver: bridge

  rabbitmq_net:
    driver: bridge

  hdfs:
    driver: bridge
  
  mongo_net:
    driver: bridge
