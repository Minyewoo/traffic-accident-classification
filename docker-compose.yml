version: "3.9"
services:
  spark:
    hostname: spark
    image: bitnami/spark:3.3.1
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_URL=spark://spark:7077
    ports:
      - 8088:8080
    networks:
      - spark_net

  spark-worker:
    image: bitnami/spark:3.3.1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=4
    networks:
      - spark_net
      - hdfs

  rabbitmq:
    hostname: rabbitmq
    image: rabbitmq:3.9.25-management-alpine
    ports:
      - 5672:5672
      - 15672:15672
    volumes:
      - ./data/rabbitmq/:/var/lib/rabbitmq/
    networks:
      - rabbitmq_net
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 300s
      timeout: 30s
      retries: 5
    
  redis:
    hostname: redis
    image: nyaaneet/rejson:v2.4.0
    ports:
      - 6379:6379
    networks:
      - redis_net
    healthcheck:
      test: [ "CMD", "redis-cli", "--raw", "incr", "ping" ]
      interval: 300s
      timeout: 30s
      retries: 5

  hdfs-namenode:
    image: gradiant/hdfs:3.2.2
    volumes:
      - ./data/hadoop_namenode:/hadoop/dfs/name
    environment:
      - HDFS_CONF_dfs_permissions_enabled=false
    command:
      - namenode
    ports:
      - 8020:8020
      - 9870:9870
    networks:
      - hdfs

  hdfs-datanode:
    image: gradiant/hdfs:3.2.2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://hdfs-namenode:8020
      - HDFS_CONF_dfs_permissions_enabled=false
    volumes:
      - ./data/hadoop_datanode:/hadoop/dfs/data
    command:
      - datanode
    networks:
      - hdfs
  
  traffic_events_crawler_msc:
    env_file:
      - ./crawler/.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=traffic_events
      - TRAFFIC_URL=https://2gis.ru/moscow?m=37.62017%2C55.755012%2F12&traffic
      - CITY_NAME=msc
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  weather_forecast_crawler_msc:
    env_file:
      - ./crawler/.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=weather_forecast
      - WFORECAST_URL=https://yandex.ru/pogoda/moscow
      - CITY_NAME=msc
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  traffic_events_crawler_spb:
    env_file:
      - ./crawler/.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=traffic_events
      - TRAFFIC_URL=https://2gis.ru/spb?m=30.40821%2C59.953673%2F10.94&traffic
      - CITY_NAME=spb
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  weather_forecast_crawler_spb:
    env_file:
      - ./crawler/.env
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RABBITMQ_HOST=rabbitmq
      - CRAWLER_TYPE=weather_forecast
      - WFORECAST_URL=https://yandex.ru/pogoda/saint-petersburg
      - CITY_NAME=spb
    build:
      context: ./crawler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
      - redis
    networks:
      - redis_net
      - rabbitmq_net

  scheduler:
    env_file:
      - ./scheduler/.env
    environment:
      - RABBITMQ_HOST=rabbitmq
    build:
      context: ./scheduler/
      dockerfile: Dockerfile
    restart: on-failure
    depends_on:
      - rabbitmq
    networks:
      - rabbitmq_net

  data-preprocessor:
    build:
      context: ./data-preprocessor/
      dockerfile: Dockerfile
    volumes:
      - ./data-preprocessor/src:/app:ro
    environment:
      - PROCESSING_SCHEDULER_EXCHANGE_NAME=${PROCESSING_SCHEDULER_EXCHANGE_NAME}
      - SPARK_SUBNET=${SPARK_SUBNET}
      - HDFS_URL=hdfs://hdfs-namenode:8020
      - HDFS_BLOCK_SIZE=${HDFS_BLOCK_SIZE}
      - TRAFFIC_DATA_SAVING_PATH=processed/data-traffic.csv
      - TRAFFIC_COUNT_DATA_SAVING_PATH=processed/data-traffic-count.csv
    restart: on-failure
    networks:
      - spark_net
      - rabbitmq_net
      - redis_net
      - hdfs

  model:
    build:
      context: ./model/
      dockerfile: Dockerfile
    volumes:
      - ./model/src:/app:ro
    environment:
      - SPARK_SUBNET=${SPARK_SUBNET}
      - HDFS_URL=hdfs://hdfs-namenode:8020
      - HDFS_BLOCK_SIZE=${HDFS_BLOCK_SIZE}
      - TRAFFIC_DATA_SAVING_PATH=processed/data-traffic.csv
      - TRAFFIC_COUNT_DATA_SAVING_PATH=processed/data-traffic-count.csv
    restart: on-failure
    networks:
      - spark_net
      - rabbitmq_net
      - redis_net
      - hdfs

networks:
  spark_net:
    driver: bridge
    ipam:
      config:
        - subnet: ${SPARK_SUBNET}
          gateway: ${SPARK_GATEAWAY}
  
  redis_net:
    driver: bridge

  rabbitmq_net:
    driver: bridge

  hdfs:
    driver: bridge
